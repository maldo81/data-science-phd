---
title: "Cleaning your Data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Already complete Code

As we move on from week to week and task to task, the code that you have already completed, will stay on the template but will not run, this is possible by adding eval=FALSE to the corresponding code chunk.
Note that the libraries need to be linked to this program as well.


```{r install}

# Install and load necessary libraries
#install.packages("ggplot2") # Install ggplot2 for plotting, if you have already installed the packages, comment this out by enterring a # in front of this command
#install.packages("scales")  # Install scales for formatting
#install.packages("moments") # Install moments for skewness and kurtosis
library(ggplot2)            # Load ggplot2 library
library(scales)             # Load scales library


```


## Setting up your directory in your computer.

This needs to be addressed here.



```{r Dir}
# Check the current working directory
getwd()

# in the next line, change the directory to the place where you saved the
# data file, if you prefer you can save your data.csv file in the directory
# that command 7 indicated.
# for example your next line should like something similar to this: setwd("C:/Users/tsapara/Documents")

# Set the working directory to where the data file is located
# This ensures the program can access the file correctly

setwd("C:/Users/JuanMaldonado/OneDrive/University/National University/DDS-8500 v1 Principles of Data Science/Week 4/Assignment 4")

### Choose an already existing directory in your computer.
```

## Setting up your personalized data

```{r Read, eval=FALSE}
# Read the CSV file
# The header parameter ensures column names are correctly read
# sep defines the delimiter (comma in this case)
# stringsAsFactors prevents automatic conversion of strings to factors
df <- read.csv("data.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)

##########################################################
# Define variables A and B based on your student ID
# A represents the first 3 digits, B represents the last 3 digits
A <- 967
B <- 612
Randomizer <- A + B # Randomizer ensures a consistent seed value for reproducibility


# Generate a random sample of 500 rows from the dataset
set.seed(Randomizer) # Set the seed for reproducibility
sample_size <- 500
df <- df[sample(nrow(df), sample_size, replace = TRUE), ] # Sample the dataset

write.csv(df, file = "my_data.csv", row.names = FALSE) # this command may take some time to run once it is done, it will create the desired data file locally in your directory


```


## Knit your file

As practice, you may want now to knit your file in an html. To do this, you should click on the knit button on the top panel, and wait for the rendering file. The HTML will open once it is done for you to review.

It is recommended to practice with RMD and download and review the following cheatsheets: https://rmarkdown.rstudio.com/lesson-15.HTML

In addition, you may want to alter some of the editor components and re-knit your file to gain some knowledge and understanding of RMD.
For a complete tutorial, visit: https://rmarkdown.rstudio.com/lesson-2.html


```{r ReadYourFile}

df <- read.csv("my_data.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
```


## Cleaning up your data


***Step 0. Now that you read  the file, you want to learn few information about your data***

The following commands will not be explained here, do your research, review your csv file and answer the questions related with this part of your code.


```{r Explore}

# Basic exploratory commands
nrow(df)       # Number of rows in the dataset
length(df)     # Number of columns (or variables) in the dataset
str(df)        # Structure of the dataset (data types and a preview)
summary(df)    # Summary statistics for each column

```

```{r stats_raw}
# Key metadata and missingness before cleanup
file_format <- "CSV"
has_headers <- TRUE
obs <- nrow(df)
vars <- ncol(df)
na_total <- sum(is.na(df))
blank_total <- sum(sapply(df, function(col) sum(trimws(as.character(col)) == "")))
data_types <- sapply(df, class)
numeric_vars <- names(df)[sapply(df, is.numeric)]
categorical_vars <- names(df)[!sapply(df, is.numeric)]
id_is_numeric <- is.numeric(df$ID)
```


## Your Turn

Please answer the following questions, by typing information after the question.

***Step 0 Answers***

File format and command: The file is in `r file_format` format and was read with `read.csv("my_data.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)`.  
Headers: `r has_headers`.  
Observations: `r obs`.  
Variables: `r vars`.  
Purpose of `str(df)`: It displays the structure of the dataframe, including variable types and sample values, to verify correct import.  
Meaning of `summary(df)`: It provides descriptive statistics for numeric variables and level counts for factors, plus NA counts, to quickly assess distributions and missingness.

***Question 1***

**What type of variables does your file include?**

***Answer 1:*** The dataset includes both numeric and categorical variables. Numeric variables are `r paste(numeric_vars, collapse = ", ")`, while categorical variables are `r paste(categorical_vars, collapse = ", ")`.



***Question 2***

**Specific data types?**

***Answer 2:*** Most non-numeric fields are read as factors because `stringsAsFactors = TRUE`. The numeric fields are stored as integer or numeric/double types. Use `str(df)` to verify the class of each column.




***Question 3***

**Are they read properly?**

***Answer 3:*** The data are read with headers and expected data types for most fields. `ID` should be numeric; if `r id_is_numeric` is `FALSE`, re-check the source file and re-run the personalization step to ensure columns are aligned correctly.



***Question 4***

**Are there any issues ?**

***Answer 4:*** Missing values are present as both explicit `NA` tokens and blank strings. If `ID` is not numeric or any column looks shifted, that indicates a data alignment issue that should be fixed by re-reading the correct `data.csv` and regenerating `my_data.csv`.



***Question 5***

**Does your file includes both NAs and blanks?**

***Answer 5:*** Yes. There are `r na_total` explicit `NA` values and `r blank_total` blank entries before cleanup.



***Question 6***

**How many NAs do you have and **

***Answer 6:*** `r na_total` total NAs before converting blanks to NAs.



***Question 7***

**How many blanks? **

***Answer 7:*** `r blank_total` total blank entries before cleanup.




## Cleanup Continued

***Step 1:  Handling both blanks and NAs is not simple so first we want to eliminate some of those, let's eliminate the blanks and change them to NAs***


```{r Cleanup}


#
# Step 1:  # Handling both blanks and NAs is not simple so first we want to eliminate
# some of those, let's eliminate the blanks and change them to NAs
#


# Replace blanks with NAs across the dataset
# This ensures that blank values are consistently treated as missing data
df[df == ""] <- NA

# Convert specific columns to factors
# This step ensures categorical variables are treated correctly after replacing blanks
factor_columns <- c("Gender", "Education", "Score", "MaritalStatus", "Category", 
                    "Employment", "Color", "Hobby", "Location")
df[factor_columns] <- lapply(df[factor_columns], function(col) as.factor(as.character(col)))

```

```{r stats_step1}
# Missingness after converting blanks to NAs
na_total_step1 <- sum(is.na(df))
blank_total_step1 <- sum(sapply(df, function(col) sum(trimws(as.character(col)) == "")))
```

```{r summary_tables}
# Summary tables for numeric and categorical variables after Step 1
numeric_summary <- data.frame(
  Variable = numeric_vars,
  Mean = sapply(df[numeric_vars], function(x) mean(x, na.rm = TRUE)),
  Median = sapply(df[numeric_vars], function(x) median(x, na.rm = TRUE))
)

categorical_summary <- do.call(
  rbind,
  lapply(categorical_vars, function(v) {
    tab <- table(df[[v]], useNA = "ifany")
    data.frame(Variable = v, Level = names(tab), Count = as.integer(tab))
  })
)

knitr::kable(numeric_summary, caption = "Numeric Variables: Mean and Median (after Step 1)")
knitr::kable(categorical_summary, caption = "Categorical Variables: Level Counts (after Step 1)")
```


## Cleanup Continued

***Step 2: Count NAs in the entire dataset***


```{r CheackNAs}


#
# Step 2: Count NAs in the entire dataset


# Count the total number of NAs in the dataset
total_nas <- sum(is.na(df))
total_nas # Print the total number of missing values

```

## Your Turn

Please answer the following questions, by typing information after the question.

***Question 8***

**Explain what the printed number is, what is the information that relays and how can you use it in your analysis?**

***Answer 8:*** `total_nas` is the total number of missing values in the dataset after converting blanks to NAs. It shows the overall missingness burden and helps decide whether to drop records, drop variables, or impute values.





## Clean Up Continued

***Step 3: Count rows with NAs.***


```{r NARows }

#
# Step 3: Count rows with NAs
#

# Count rows with at least one NA
rows_with_nas <- sum(rowSums(is.na(df)) > 0)
Percent_row_NA <- percent(rows_with_nas / nrow(df)) # Percentage of rows with NAs
rows_with_nas
Percent_row_NA

```

## Your Turn

***Question 9***

**How large is the proportion of the rows with NAs, we can drop up to 5%?**

***Answer 9:*** The proportion of rows with at least one NA is `r Percent_row_NA`. This exceeds 5% if the value is greater than 5%.



***Question 10***

**Do you think that would be wise to drop the above percent? **

***Answer 10:*** No. Dropping all rows with NAs would remove `r rows_with_nas` rows (about `r Percent_row_NA` of the data), which is too large and could bias the dataset.



***Question 11***

**How this will affect your dataset? **

***Answer 11:*** It would reduce the dataset to about `r nrow(df) - rows_with_nas` rows and could distort distributions and relationships if missingness is not completely random.




## CleanUp Continued

***Step 4: Count columns with NAs***


```{r NAColumns}



#  
# Step 4: Count columns with NAs

# Count columns with at least one NA
cols_with_nas <- sum(colSums(is.na(df)) > 0)
Percent_col_NA <- percent(cols_with_nas / length(df)) # Percentage of columns with NAs
cols_with_nas
Percent_col_NA

```

## Your Turn

***Question 12***

**How large is the proportion of the cols with NAs, we never want to drop entire columnes as this would mean that we will loose variables and associations but do you think that would be wise to drop the above percent?**

***Answer 12:*** The proportion of columns with NAs is `r Percent_col_NA` (`r cols_with_nas` columns). Dropping columns is not advisable here because it would remove important variables and weaken analysis.



***Question 13***

**How this will affect your dataset? **

***Answer 13:*** Dropping columns would shrink the feature set to `r length(df) - cols_with_nas` columns and could eliminate key predictors or outcomes, reducing analytical value.





## Imputation

***Step 5: Replace NAs with appropriate values (mean for numeric and integer,mode for factor, "NA" for character)***

In later weeks we will learn how to replace the NAs properly based on the descriptive statistics and you will discuss this code.For now, you can assume that by setting the mean of the variable for numeric and mode for categorical it is correct - this is not always the case of course but the code will become much more complicated in that case.


```{r Imputation}


# 
# Step 5: Replace NAs with appropriate values (mean for numeric and integer,
# mode for factor, "NA" for character)
# In later weeks we will learn how to replace the NAs properly based on the
# descriptive statistics and you will discuss this code.
# for now, you can assume that by setting the mean of the variable for numeric
# and mode for categorical it is correct - this is not always the case of course
# but the code will become much more complicated in that case.


# Replace NAs with appropriate values
# Numeric: Replace with the mean if sufficient data is available
# Categorical: Replace with the mode (most common value)
# Character: Replace with the string "NA"
df <- lapply(df, function(col) {
  if (is.numeric(col) || is.integer(col)) { # Numeric or integer columns
    if (sum(!is.na(col)) > 10) {
      col[is.na(col)] <- mean(col, na.rm = TRUE) # Replace with mean
    } else {
      col[is.na(col)] <- approx(seq_along(col), col, n = length(col))[["y"]][is.na(col)] # Interpolation
    }
  } else if (is.factor(col)) { # Factor columns
    mode_val <- names(sort(-table(col)))[1] # Mode (most common value)
    col[is.na(col)] <- mode_val
  } else if (is.character(col)) { # Character columns
    col[is.na(col)] <- "NA" # Replace with "NA"
  }
  return(col) # Return the modified column
})

df <- as.data.frame(df) # Convert the list back to a dataframe


#
# following the above method to impute, has now changed some of the statistics


# Check the updated dataset and ensure no remaining NAs
summary(df)

```


## Your Turn

**Essay Question**

Run summary(df) and compare with the previous statistics. Do you observe any undesired changes? Explain in detail. Are there any more NA's in your file?What is the information that is printed by the summary? How can this be interpreted? what are your observations? Verify the effects of imputation, and explain in detail. Compare the updated summary with the earlier statistics and note changes. Explain everything that you obsevrve.

**Answer**
The `summary(df)` output reports descriptive statistics for each variable. For numeric variables it shows min, quartiles, median, mean, and max; for factors it shows counts by level; and it also reports the number of NAs. After Step 1, blanks are converted to NAs, so the total missing count increases from `r na_total` to `r na_total_step1`, which better reflects true missingness.

Step 5 uses mean imputation for numeric variables and mode imputation for categorical variables, and uses linear interpolation only when there are too few numeric values. Linear interpolation fills missing points by connecting adjacent observed values with a straight line. This can be reasonable for ordered numeric data (like time series) but can be inappropriate when the values are not ordered in time or if the missingness pattern is not linear.

After imputation, `summary(df)` should show no remaining NAs. You may observe shifts in means, medians, and category proportions because mean and mode imputation pull values toward the center or the most common category, which can shrink variance and inflate dominant categories. These changes could be avoided by using more appropriate imputation methods such as multiple imputation (MICE), k-nearest neighbors, or domain-specific rules, and by analyzing missingness mechanisms (MCAR, MAR, MNAR) before choosing a method.
